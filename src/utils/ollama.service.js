const axios = require('axios');

const logger = require('../../../utils/logger.utils');
const { REPORT_PROMPT } = require('./prompts');

// ğŸ”¥ Ø·Ø§Ø¨ÙˆØ± Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±: Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… ØªØ´ØºÙŠÙ„ Ø£ÙƒØ«Ø± Ù…Ù† Ø¹Ù…Ù„ÙŠØ© AI ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
let requestQueue = Promise.resolve();

exports.generateReportContent = async (targetUrl, cleanedData) => {
    // Ù†Ø±Ø¨Ø· Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¨Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ø³Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
    const currentTask = async () => {
    
    // 1. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø£ÙˆØ§Ù…Ø± (Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠ)
    const today = new Date().toISOString().split('T')[0];
    const prompt = REPORT_PROMPT
        .replace('{{DATA}}', JSON.stringify(cleanedData, null, 2))
        .replace('{{TARGET_URL}}', targetUrl)
        .replace('{{DATE}}', today)
        .replace('{{END_DATE}}', today)
        .replace('{{TIMESTAMP}}', Date.now().toString());

    try {
        if (logger) logger.info(`ğŸ¤– Generating Professional Security Report (Maximum Quality Mode) for: ${targetUrl}`);
        const startTime = Date.now();

        // Ø­Ø³Ø§Ø¨ ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ø­Ø¬Ù… Ø§Ù„Ø¯Ø§ØªØ§
        const dataStr = JSON.stringify(cleanedData);
        if (dataStr.length > 10000) if (logger) logger.warn("âš ï¸ Heavy Input Data: Processing might take extra time.");

        const response = await axios.post('http://localhost:11434/api/generate', {
            model: "llama3.1:8b-instruct-q4_0", 
            prompt: prompt,
            stream: false,
            
            // ğŸ¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ù‚ØµÙˆÙ‰ (Maximum Quality & Professionalism)
            // Ø§Ù„Ù‡Ø¯Ù: ØªÙ‚Ø±ÙŠØ± Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¬Ø¯Ø§Ù‹ Ø¨ØºØ¶ Ø§Ù„Ù†Ø¸Ø± Ø¹Ù† Ø§Ù„ÙˆÙ‚Øª (Hybrid Mode)
            options: { 
                // 1. ğŸ“Š Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Context Window)
                // Ø±ÙØ¹Ù†Ø§ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ù€ 8192 Ø¹Ø´Ø§Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠÙ‚Ø±Ø£ ÙƒÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆÙŠÙƒØªØ¨ ØªÙ‚Ø±ÙŠØ± Ø·ÙˆÙŠÙ„ ÙˆÙ…ØªØ±Ø§Ø¨Ø·
                num_ctx: 8192,         
                
                // 2. ğŸ”¥ GPU + CPU (Hybrid Mode)
                // ÙƒØ±Øª Ø§Ù„Ø´Ø§Ø´Ø© 4GB Ù„Ø§ ÙŠÙƒÙÙŠ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (4.7GB).
                // Ø§Ù„Ø­Ù„: Ù†Ø¶Ø¹ 12 Ø·Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ±Øª (Ø¹Ø´Ø§Ù† Ù†Ø³ÙŠØ¨ Ù…Ø³Ø§Ø­Ø© Ù„Ù„Ø°Ø§ÙƒØ±Ø©) ÙˆØ§Ù„Ø¨Ø§Ù‚ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ i5-12500 Ø§Ù„Ù‚ÙˆÙŠ.
                num_gpu: 12,           
                
                // 3. ğŸ¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø© (Professional Tone)
                temperature: 0.1,      // Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ£Ù„ÙŠÙ
                top_p: 0.9,            
                top_k: 40,             
                repeat_penalty: 1.1,   // Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±
                
                // 4. ğŸ“ Ø§Ù„Ø£Ø¯Ø§Ø¡ (CPU Optimization)
                // Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ i5-12500 ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 6 Ø£Ù†ÙˆÙŠØ© PerformanceØŒ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙƒÙ„Ù‡Ø§
                num_thread: 6,         
                num_predict: -1,       // Ø³ÙŠØ¨Ù‡ ÙŠÙƒØªØ¨ Ø¨Ø±Ø§Ø­ØªÙ‡ Ù„Ø­Ø¯ Ù…Ø§ ÙŠØ®Ù„Øµ Ø§Ù„ÙÙƒØ±Ø© (Unlimited)
                
                // 5. ğŸ”¥ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙ‚Ù†ÙŠØ©
                num_batch: 512,        
                use_mmap: true,        
                num_keep: 24,          // Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø³ÙŠØ§Ù‚ Ø£ÙƒØ¨Ø± Ù„Ø¶Ù…Ø§Ù† ØªØ±Ø§Ø¨Ø· Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            } 
        }, {
            timeout: 1200000,  // 20 Ø¯Ù‚ÙŠÙ‚Ø© - ÙˆÙ‚Øª ÙƒØ§ÙÙŠ Ø¬Ø¯Ø§Ù‹
            maxContentLength: Infinity,
            maxBodyLength: Infinity
        });

        if (response.data && response.data.response) {
            const duration = ((Date.now() - startTime) / 1000).toFixed(2);
            if (logger) logger.info(`âœ… Security Report Generated Successfully (Max Quality Mode) in ${duration}s ğŸ’`);
            console.log(`ğŸ’ AI Analysis Time: ${duration}s (Professional Hybrid Mode - i5+RTX3050)`);
            
            const timestamp = new Date().toLocaleString('en-US', { timeZone: 'Africa/Cairo' });
            
            const reportWithMetadata = `---
Report Generated: ${timestamp}
Target: ${targetUrl}
Analysis Engine: VulnCraft AI (Maximum Quality - Hybrid Architecture)
Processing Time: ${duration}s
Report Quality: â˜…â˜…â˜…â˜…â˜… Professional Security Analysis
Confidentiality: Internal / Restricted
---

${response.data.response}

---
<div align="center">
<strong>VulnCraft Project</strong> â€¢ <em>Next-Gen Security Analysis</em>
</div>
`;
            return reportWithMetadata;
        } else {
            throw new Error("Received empty response from AI Model");
        }

    } catch (error) {
        const errMsg = error.message;
        
        if (errMsg.includes("404")) {
            console.error("âŒ Model not found! Run: ollama pull llama3.1:8b-instruct-q4_0");
        } else if (errMsg.includes("timeout")) {
            console.error("â±ï¸ Timeout! Your report might be too complex.");
            console.error("   Solutions:");
            console.error("   1. This is normal for 7+ vulnerabilities");
            console.error("   2. Current timeout: 15 minutes");
            console.error("   3. Consider splitting large scans");
        } else if (errMsg.includes("out of memory") || errMsg.includes("CUDA") || errMsg.includes("OOM")) {
            console.error("ğŸ’¾ GPU Out of Memory! Solutions:");
            console.error("   1. Change num_gpu from -1 to 25");
            console.error("   2. Reduce num_ctx to 1536");
            console.error("   3. Reduce num_batch to 1024");
            console.error("   4. Close Chrome and other GPU apps");
            console.error("   5. Run: nvidia-smi to check VRAM usage");
        }
        
        if (logger && logger.error) logger.error(`AI Service Error: ${errMsg}`);
        else console.error("Full Error:", errMsg);
        
        return `# Report Generation Failed
**Target:** ${targetUrl}
**Error:** AI Processing Error (Hybrid Mode)
**Details:** ${errMsg}

**Troubleshooting Tips:**
1. Model: llama3.1:8b-instruct-q4_0 âœ…
2. Check VRAM: 4GB is tight for 8192 context.
3. Try reducing num_ctx to 4096 if OOM occurs.
4. Current settings: num_gpu=12 (Hybrid), num_thread=6`;
    }
    };

    const result = requestQueue.then(currentTask);
    requestQueue = result.catch(() => {});

    return result;
};